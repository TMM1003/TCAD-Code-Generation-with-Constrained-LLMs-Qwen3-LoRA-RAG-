{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64915fe3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os, json, torch, inspect\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "DATA_PATH = \"./data/silvaco_dataset_train.json\"\n",
    "OUT_DIR = \"checkpoints/qwen3-0.6b-lora\"\n",
    "SEED = 42\n",
    "\n",
    "# LoRA targets and hyperparams\n",
    "TARGETS = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "LORA_R, LORA_A, LORA_D = 16, 32, 0.05\n",
    "\n",
    "EPOCHS, LR, WARMUP = 3, 1e-4, 0.05\n",
    "BATCH, ACCUM, MAXLEN = 4, 16, 2048   # effective batch ~64\n",
    "\n",
    "\n",
    "# Tokenizer & model\n",
    "tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "if tok.pad_token is None:\n",
    "    # Qwen often lacks a pad token\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# LoRA config\n",
    "peft_cfg = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_A,\n",
    "    lora_dropout=LORA_D,\n",
    "    target_modules=TARGETS,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, peft_cfg)\n",
    "\n",
    "# Data loading / preprocessing\n",
    "def format_example(ex):\n",
    "    return {\n",
    "        \"text\": ex[\"instruction\"].strip() + \"\\n\\n\" + ex[\"output\"].strip()\n",
    "    }\n",
    "\n",
    "raw = load_dataset(\"json\", data_files=DATA_PATH, split=\"train\").shuffle(seed=SEED)\n",
    "splits = raw.train_test_split(test_size=0.2, seed=SEED)\n",
    "train_ds = splits[\"train\"].map(format_example)\n",
    "val_ds = splits[\"test\"].map(format_example)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tok(\n",
    "        batch[\"text\"],\n",
    "        max_length=MAXLEN,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "train_tok = train_ds.map(\n",
    "    tokenize, batched=True, remove_columns=train_ds.column_names\n",
    ")\n",
    "val_tok = val_ds.map(\n",
    "    tokenize, batched=True, remove_columns=val_ds.column_names\n",
    ")\n",
    "\n",
    "collator = DataCollatorForLanguageModeling(tok, mlm=False)\n",
    "\n",
    "# TrainingArguments – portable across GPU / CPU and HF versions\n",
    "\n",
    "# Hardware / precision detection\n",
    "use_cuda = torch.cuda.is_available()\n",
    "bf16_supported = (\n",
    "    use_cuda\n",
    "    and hasattr(torch.cuda, \"is_bf16_supported\")\n",
    "    and torch.cuda.is_bf16_supported()\n",
    ")\n",
    "\n",
    "# Introspect TrainingArguments signature (handles version differences)\n",
    "ta_sig = inspect.signature(TrainingArguments)\n",
    "\n",
    "train_kwargs = dict(\n",
    "    output_dir=OUT_DIR,\n",
    "    per_device_train_batch_size=BATCH,\n",
    "    per_device_eval_batch_size=BATCH,\n",
    "    gradient_accumulation_steps=ACCUM,\n",
    "    learning_rate=LR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    warmup_ratio=WARMUP,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=0.1,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    gradient_checkpointing=True,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "# evaluation / eval_strategy renamed across transformers versions\n",
    "if \"eval_strategy\" in ta_sig.parameters:\n",
    "    train_kwargs[\"eval_strategy\"] = \"epoch\"\n",
    "elif \"evaluation_strategy\" in ta_sig.parameters:\n",
    "    train_kwargs[\"evaluation_strategy\"] = \"epoch\"\n",
    "\n",
    "# Precision flags: only set what the machine actually supports\n",
    "if \"bf16\" in ta_sig.parameters and bf16_supported:\n",
    "    # e.g. newer GPUs with bf16\n",
    "    train_kwargs[\"bf16\"] = True\n",
    "elif \"fp16\" in ta_sig.parameters and use_cuda:\n",
    "    # e.g. RTX 2060: fp16 yes, bf16 no\n",
    "    train_kwargs[\"fp16\"] = True\n",
    "# else: CPU only → stay in full precision\n",
    "\n",
    "args = TrainingArguments(**train_kwargs)\n",
    "\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tok,\n",
    "    args=args,\n",
    "    data_collator=collator,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(OUT_DIR)   # saves adapters in this dir\n",
    "tok.save_pretrained(OUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
