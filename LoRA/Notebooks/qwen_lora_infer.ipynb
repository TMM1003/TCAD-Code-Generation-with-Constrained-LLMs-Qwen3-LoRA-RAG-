{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ca996d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# qwen3_lora_infer.py\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "BASE_MODEL_NAME = \"Qwen/Qwen3-0.6B\"\n",
    "ADAPTER_DIR = \"checkpoints/qwen3-0.6b-lora\"\n",
    "\n",
    "# Device / precision setup\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = \"cuda\" if use_cuda else \"cpu\"\n",
    "\n",
    "# bf16 is only on newer GPUs; for portability, fall back to fp16/fp32\n",
    "bf16_supported = (\n",
    "    use_cuda\n",
    "    and hasattr(torch.cuda, \"is_bf16_supported\")\n",
    "    and torch.cuda.is_bf16_supported()\n",
    ")\n",
    "\n",
    "if bf16_supported:\n",
    "    load_dtype = torch.bfloat16\n",
    "elif use_cuda:\n",
    "    load_dtype = torch.float16\n",
    "else:\n",
    "    load_dtype = torch.float32\n",
    "\n",
    "\n",
    "# Tokenizer\n",
    "\n",
    "# Load tokenizer from adapter dir so you get the same vocab/settings as training\n",
    "tok = AutoTokenizer.from_pretrained(ADAPTER_DIR, use_fast=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "\n",
    "# Base model + LoRA adapters\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    torch_dtype=load_dtype,\n",
    "    device_map=\"auto\" if use_cuda else None,  # let HF place on GPU if available\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, ADAPTER_DIR)\n",
    "model.eval()\n",
    "\n",
    "# If we're on pure CPU and didn't use device_map, move to device explicitly\n",
    "if not use_cuda:\n",
    "    model.to(device)\n",
    "\n",
    "# Make sure model has pad/eos ids set consistently\n",
    "if model.config.pad_token_id is None:\n",
    "    model.config.pad_token_id = tok.pad_token_id\n",
    "if model.config.eos_token_id is None:\n",
    "    model.config.eos_token_id = tok.eos_token_id\n",
    "\n",
    "# Inference\n",
    "# Change spec to whatever prompt you want to test\n",
    "spec = (\n",
    "    \"Generate a Silvaco/SmartSpice deck for an NMOS L=180nm W=1um, \"\n",
    "    \"VDD=1.8V. Include mesh, models, contacts, bias sources, and a DC sweep \"\n",
    "    \"of VGS from 0 to 1.8V.\"\n",
    ")\n",
    "\n",
    "inputs = tok(spec, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=800,\n",
    "        do_sample=False,\n",
    "        eos_token_id=tok.eos_token_id,\n",
    "        pad_token_id=tok.pad_token_id,\n",
    "    )\n",
    "\n",
    "print(tok.decode(output_ids[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
